##
##
##

# https://arxiv.org/abs/2109.08203
seed: 3407
work-dir: '/path/to/work_dir' # directory where to save all results of trainings and tests (default: ./work_dir)

# list of datasets to generate or to use for training/testing
datasets:
  - name: 'ntu60-xsub' # name of the dataset (options: ntu60-xsub, ntu60-xview, ntu120-xsub, ntu120-xset)
    dataset_path: 'data/ntu60-xsub' # path where the processed dataset will be saved or can be found
    training_args:
      normalize: True # whether or not to standardize the dataset (default: False)
      change_probability: 0.15 # probability that a joint will be changed
      shuffle_probability: 0.15 # probability that a joint will be shuffled
      shuffle_length: 3 # how many contiguous joints can be shuffled
    generate_args: # options used for dataset generation
      ignored_file: 'data/ignore.txt' # file with list of samples to ignore
      ntu60_path: 'data/nturgbd_skeletons_s001_to_s017'
      num_frames: 64 # number of frames that will be saved for each sample

# list of models for training and testing
models:
  - name: 'MS-STED' # name of the model
    architecture: 'path/to/architecture/config.yaml' # path to the architecture  

# currently not supported
tests:
  - batch_size: 64

# list of trainings
trainings: 
  - model: 0 # index of the model to use
    dataset: 0 # index of the dataset to use
    resume: 'p-1' # resume pretraining after epoch 1
    # work-dir: 'path/to/work-dir' # this is optional except if you want to resume training
    # seed: 10 # changed seed for this training
    pretraining_args:
      train_batch_size: 64
      eval_batch_size: 64
      accumulation_steps: 16
      num_epochs: 3
      save_interleave: 1 # save checkpoint afterhow many epochs from the previous checkpoint
      optimizer: 0 # index of the optimizer to use
      scheduler: 0 # index of the LR scheduler to use
      reconstruction-lambda: 0.5 # default 1.0
      discrimination-lambda: 0.1 # default 1.0
    classification_args:
      train_batch_size: 64
      eval_batch_size: 64
      accumulation_steps: 16
      num_epochs: 3
      save_interleave: 1
      optimizer: 0
      scheduler: 0
      label_smoothing: 0.1

# list of optimizers to use for training
optimizers: 
  - name: 'AdamW' # name of the optimizer (this is the name of the pytorch class)
    arsg:
      lr: 0.01
  - name: 'SGD'
    # parameters that will be passed to the constructor
    args:
      lr: 0.1
      momentum: 0.9
      nesterov: False
      weight_decay: 0.0001

# list of LR schedulers
lr_schedulers: 
  - name: 'LinearLR' # name of the scheduler (this is the name of the pytorch class)
    after_batch: False # whether or not scheduler.step() must be called after each batch or at the end of an epoch (default: False)
  - name: 'OneCycleLR'
    after_batch: True
    # parameters that will be passed to the constructor
    arsg:
      max_lr: 1.0
      epochs: 64
      steps_per_epoch: 10022
