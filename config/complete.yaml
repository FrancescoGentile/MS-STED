##
##
##

# list of gpus to use for training and testings
# at the moment, only one gpu is supported
gpus: 
  - 0
# https://arxiv.org/abs/2109.08203
seed: 3407
work-dir: '/path/to/work_dir' # directory where to save all results of trainings and tests (default: ./work_dir)

# list of datasets to generate or to use for training/testing
datasets:
  - name: 'ntu60-xsub' # name of the dataset (options: ntu60-xsub, ntu60-xview, ntu120-xsub, ntu120-xset)
    dataset_path: 'data/ntu60-xsub' # path where the processed dataset will be saved or can be found
    normalize: True # whether or not to standardize the dataset (default: False)
    generate_args: # options used for dataset generation
      ignored_file: 'data/ignore.txt' # file with list of samples to ignore
      ntu60_path: 'data/nturgbd_skeletons_s001_to_s017'
      num_frames: 64 # number of frames that will be saved for each sample

# list of models for training and testing
models:
  - name: 'MS-STED' # name of the model
    architecture: 'path/to/architecture/config'
    pretrained: '/path/to/weights.pth' # mandatory field for testing

# currently not supported
tests:
  - batch_size: 64

# list of trainings
trainings: 
  - model: 0 # index of the model to use
    dataset: 0 # index of the dataset to use
    optimizer: 0 # index of the optimizer to use
    scheduler: 0 # index of the LR scheduler to use
    train_batch_size: 2
    eval_batch_size: 1
    max_epoch: 64 # number of epochs
    resume: False 
# inside each training, you can specify different work_dir, gpus and seed options

# list of optimizers to use for training
optimizers: 
  - name: 'AdamW' # name of the optimizer (this is the name of the pytorch class)
    lr: 0.01
  - name: 'SGD'
    # parameters that will be passed to the constructor
    lr: 0.1
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0001

# list of LR schedulers
lr_schedulers: 
  - name: 'LinearLR' # name of the scheduler (this is the name of the pytorch class)
    after_batch: False # whether or not scheduler.step() must be called after each batch or at the end of an epoch (default: False)
  - name: 'OneCycleLR'
    after_batch: True
    # parameters that will be passed to the constructor
    max_lr: 1.0
    epochs: 64
    steps_per_epoch: 10022
